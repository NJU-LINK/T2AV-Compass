#!/usr/bin/env python3
"""
æ¨¡å‹å¯¹æ¯”åˆ†æè„šæœ¬

ç”¨äºå¯¹æ¯”ä¸åŒæ¨¡å‹ï¼ˆSORA2 vs VEO3ç­‰ï¼‰çš„éŸ³é¢‘-è§†é¢‘ä¸€è‡´æ€§ç›¸ä¼¼åº¦ç»“æœã€‚

ç”¨æ³•:
  python compare_models.py --sora2_results batch_results_sora2/batch_results.json \
                           --veo3_results batch_results_veo3/batch_results.json \
                           --output comparison_report.txt
"""

import json
import argparse
import numpy as np
import pandas as pd
from pathlib import Path
from typing import Dict, List, Tuple
from datetime import datetime


def load_batch_results(json_file: str) -> Dict:
    """åŠ è½½æ‰¹é‡æµ‹è¯•ç»“æœ"""
    with open(json_file, 'r', encoding='utf-8') as f:
        return json.load(f)


def extract_similarities(results: Dict) -> List[float]:
    """ä»ç»“æœä¸­æå–ç›¸ä¼¼åº¦åˆ—è¡¨"""
    similarities = []
    for result in results['results']:
        if result['success'] and 'similarity' in result['metrics']:
            similarities.append(result['metrics']['similarity'])
    return similarities


def calculate_stats(similarities: List[float]) -> Dict:
    """è®¡ç®—ç»Ÿè®¡æ•°æ®"""
    if not similarities:
        return {}
    
    return {
        'count': len(similarities),
        'mean': float(np.mean(similarities)),
        'std': float(np.std(similarities)),
        'min': float(np.min(similarities)),
        'max': float(np.max(similarities)),
        'median': float(np.median(similarities)),
        'q1': float(np.percentile(similarities, 25)),
        'q3': float(np.percentile(similarities, 75))
    }


def compare_models(model_results: Dict[str, Dict]) -> Dict:
    """å¯¹æ¯”å¤šä¸ªæ¨¡å‹"""
    comparison = {}
    
    for model_name, results in model_results.items():
        similarities = extract_similarities(results)
        comparison[model_name] = {
            'results': results,
            'similarities': similarities,
            'stats': calculate_stats(similarities)
        }
    
    return comparison


def generate_comparison_report(comparison: Dict, output_file: str = None) -> str:
    """ç”Ÿæˆå¯¹æ¯”æŠ¥å‘Š"""
    report = []
    report.append("=" * 100)
    report.append("éŸ³é¢‘-è§†é¢‘ä¸€è‡´æ€§æ¨¡å‹å¯¹æ¯”åˆ†ææŠ¥å‘Š")
    report.append("=" * 100)
    report.append(f"\nç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
    
    # æ±‡æ€»è¡¨
    report.append("=" * 100)
    report.append("æ¨¡å‹å¯¹æ¯”æ±‡æ€»")
    report.append("=" * 100)
    report.append("")
    
    # åˆ›å»ºå¯¹æ¯”è¡¨æ ¼
    headers = ["æ¨¡å‹", "æ ·æœ¬æ•°", "å¹³å‡ç›¸ä¼¼åº¦", "æ ‡å‡†å·®", "æœ€å°å€¼", "æœ€å¤§å€¼", "ä¸­ä½æ•°"]
    report.append(f"{headers[0]:<15} {headers[1]:<10} {headers[2]:<15} {headers[3]:<12} {headers[4]:<12} {headers[5]:<12} {headers[6]:<10}")
    report.append("-" * 100)
    
    stats_list = []
    for model_name in sorted(comparison.keys()):
        data = comparison[model_name]
        stats = data['stats']
        
        if stats:
            row = [
                model_name[:14],
                str(stats['count']),
                f"{stats['mean']:.4f}",
                f"{stats['std']:.4f}",
                f"{stats['min']:.4f}",
                f"{stats['max']:.4f}",
                f"{stats['median']:.4f}"
            ]
            report.append(f"{row[0]:<15} {row[1]:<10} {row[2]:<15} {row[3]:<12} {row[4]:<12} {row[5]:<12} {row[6]:<10}")
            stats_list.append((model_name, stats))
    
    report.append("")
    report.append("")
    
    # è¯¦ç»†ç»Ÿè®¡
    report.append("=" * 100)
    report.append("è¯¦ç»†ç»Ÿè®¡åˆ†æ")
    report.append("=" * 100)
    report.append("")
    
    for model_name, stats in stats_list:
        report.append(f"ã€{model_name}ã€‘")
        report.append(f"  æ ·æœ¬æ•°:       {stats['count']}")
        report.append(f"  å¹³å‡ç›¸ä¼¼åº¦:   {stats['mean']:.4f}")
        report.append(f"  æ ‡å‡†å·®:       {stats['std']:.4f}")
        report.append(f"  æœ€å°å€¼:       {stats['min']:.4f}")
        report.append(f"  æœ€å¤§å€¼:       {stats['max']:.4f}")
        report.append(f"  ä¸­ä½æ•°:       {stats['median']:.4f}")
        report.append(f"  Q1 (25%):     {stats['q1']:.4f}")
        report.append(f"  Q3 (75%):     {stats['q3']:.4f}")
        report.append(f"  å››åˆ†ä½å·®:     {stats['q3'] - stats['q1']:.4f}")
        report.append("")
    
    # å¯¹æ¯”åˆ†æ
    if len(stats_list) > 1:
        report.append("=" * 100)
        report.append("å¯¹æ¯”åˆ†æ")
        report.append("=" * 100)
        report.append("")
        
        # æ‰¾å‡ºæœ€å¥½å’Œæœ€å·®çš„æ¨¡å‹
        sorted_by_mean = sorted(stats_list, key=lambda x: x[1]['mean'], reverse=True)
        best_model = sorted_by_mean[0]
        worst_model = sorted_by_mean[-1]
        
        report.append(f"ğŸ† æœ€ä½³æ¨¡å‹: {best_model[0]}")
        report.append(f"   å¹³å‡ç›¸ä¼¼åº¦: {best_model[1]['mean']:.4f}")
        report.append("")
        
        report.append(f"âš ï¸  æœ€å·®æ¨¡å‹: {worst_model[0]}")
        report.append(f"   å¹³å‡ç›¸ä¼¼åº¦: {worst_model[1]['mean']:.4f}")
        report.append("")
        
        # æ€§èƒ½å·®å¼‚
        improvement = ((best_model[1]['mean'] - worst_model[1]['mean']) / abs(worst_model[1]['mean']) * 100) if worst_model[1]['mean'] != 0 else 0
        report.append(f"æ€§èƒ½å·®å¼‚: {best_model[0]} æ¯” {worst_model[0]} é«˜å‡º {improvement:.1f}%")
        report.append("")
        
        # è´¨é‡åˆ†å¸ƒå¯¹æ¯”
        report.append("è´¨é‡åˆ†å¸ƒå¯¹æ¯” (ç›¸ä¼¼åº¦åˆ†æ¡£):")
        report.append("")
        
        quality_brackets = [
            (0.8, 1.0, "ä¼˜ç§€ (0.8-1.0)"),
            (0.6, 0.8, "è‰¯å¥½ (0.6-0.8)"),
            (0.4, 0.6, "ä¸­ç­‰ (0.4-0.6)"),
            (0.2, 0.4, "ä¸€èˆ¬ (0.2-0.4)"),
            (0.0, 0.2, "å·® (0.0-0.2)")
        ]
        
        for model_name, data in sorted(comparison.items()):
            report.append(f"  {model_name}:")
            similarities = data['similarities']
            
            for lower, upper, label in quality_brackets:
                count = sum(1 for s in similarities if lower <= s < upper)
                pct = (count / len(similarities) * 100) if similarities else 0
                bar = "â–ˆ" * int(pct / 5)
                report.append(f"    {label:<18} {count:>3} ({pct:>5.1f}%) {bar}")
            report.append("")
    
    # åŸå§‹æ•°æ®è¯¦æƒ…
    report.append("=" * 100)
    report.append("åŸå§‹æ•°æ®è¯¦æƒ…")
    report.append("=" * 100)
    report.append("")
    
    for model_name, data in sorted(comparison.items()):
        report.append(f"ã€{model_name}ã€‘ - å„æ ·æœ¬ç›¸ä¼¼åº¦:")
        report.append("")
        
        results = data['results']['results']
        similarities = data['similarities']
        
        for i, result in enumerate(results):
            if result['success'] and i < len(similarities):
                sim = similarities[i]
                # æ ¹æ®ç›¸ä¼¼åº¦è¯„çº§
                if sim >= 0.8:
                    level = "ğŸŸ¢ ä¼˜ç§€"
                elif sim >= 0.6:
                    level = "ğŸŸ¡ è‰¯å¥½"
                elif sim >= 0.4:
                    level = "ğŸŸ  ä¸­ç­‰"
                elif sim >= 0.2:
                    level = "ğŸŸ¤ ä¸€èˆ¬"
                else:
                    level = "ğŸ”´ å·®"
                
                pair_id = result['pair_id']
                audio_name = result['audio_name']
                video_name = result['video_name']
                description = result.get('description', '')
                
                report.append(f"  {pair_id:<12} {sim:.4f}  {level}  {description}")
        
        report.append("")
    
    report_text = "\n".join(report)
    
    # ä¿å­˜åˆ°æ–‡ä»¶
    if output_file:
        with open(output_file, 'w', encoding='utf-8') as f:
            f.write(report_text)
        print(f"âœ“ æŠ¥å‘Šå·²ä¿å­˜: {output_file}")
    
    return report_text


def generate_csv_comparison(comparison: Dict, output_file: str = None) -> pd.DataFrame:
    """ç”ŸæˆCSVå¯¹æ¯”è¡¨"""
    rows = []
    
    for model_name, data in sorted(comparison.items()):
        results = data['results']['results']
        
        for i, result in enumerate(results):
            if result['success']:
                row = {
                    'æ¨¡å‹': model_name,
                    'é…å¯¹ID': result['pair_id'],
                    'éŸ³é¢‘': result['audio_name'],
                    'è§†é¢‘': result['video_name'],
                    'æè¿°': result.get('description', ''),
                    'ç›¸ä¼¼åº¦': result['metrics']['similarity'],
                    'çŠ¶æ€': 'âœ“'
                }
                rows.append(row)
    
    df = pd.DataFrame(rows)
    
    if output_file:
        df.to_csv(output_file, index=False, encoding='utf-8-sig')
        print(f"âœ“ CSVå·²ä¿å­˜: {output_file}")
    
    return df


def main():
    parser = argparse.ArgumentParser(
        description="æ¨¡å‹å¯¹æ¯”åˆ†æè„šæœ¬",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹ç”¨æ³•:
  # å¯¹æ¯”ä¸¤ä¸ªæ¨¡å‹
  python compare_models.py --sora2_results batch_results_sora2/batch_results.json \
                           --veo3_results batch_results_veo3/batch_results.json
  
  # æŒ‡å®šè¾“å‡ºæ–‡ä»¶
  python compare_models.py --sora2_results batch_results_sora2/batch_results.json \
                           --veo3_results batch_results_veo3/batch_results.json \
                           --output comparison_report.txt \
                           --csv comparison_results.csv
        """
    )
    
    parser.add_argument("--sora2_results", type=str, help="SORA2ç»“æœJSONæ–‡ä»¶")
    parser.add_argument("--veo3_results", type=str, help="VEO3ç»“æœJSONæ–‡ä»¶")
    parser.add_argument("--model1_results", type=str, help="æ¨¡å‹1ç»“æœJSONæ–‡ä»¶")
    parser.add_argument("--model2_results", type=str, help="æ¨¡å‹2ç»“æœJSONæ–‡ä»¶")
    parser.add_argument("--model3_results", type=str, help="æ¨¡å‹3ç»“æœJSONæ–‡ä»¶")
    parser.add_argument("--output", type=str, default="comparison_report.txt", help="è¾“å‡ºæŠ¥å‘Šæ–‡ä»¶")
    parser.add_argument("--csv", type=str, help="è¾“å‡ºCSVæ–‡ä»¶")
    
    args = parser.parse_args()
    
    # æ”¶é›†æ‰€æœ‰æ¨¡å‹ç»“æœ
    model_results = {}
    
    if args.sora2_results and Path(args.sora2_results).exists():
        model_results['SORA2'] = load_batch_results(args.sora2_results)
    
    if args.veo3_results and Path(args.veo3_results).exists():
        model_results['VEO3'] = load_batch_results(args.veo3_results)
    
    if args.model1_results and Path(args.model1_results).exists():
        model_results['æ¨¡å‹1'] = load_batch_results(args.model1_results)
    
    if args.model2_results and Path(args.model2_results).exists():
        model_results['æ¨¡å‹2'] = load_batch_results(args.model2_results)
    
    if args.model3_results and Path(args.model3_results).exists():
        model_results['æ¨¡å‹3'] = load_batch_results(args.model3_results)
    
    if not model_results:
        print("é”™è¯¯: æœªæ‰¾åˆ°ä»»ä½•ç»“æœæ–‡ä»¶!")
        print("è¯·ä½¿ç”¨ --sora2_results, --veo3_results æˆ– --model1_results ç­‰å‚æ•°æŒ‡å®šç»“æœæ–‡ä»¶")
        return
    
    print(f"åŠ è½½äº† {len(model_results)} ä¸ªæ¨¡å‹çš„ç»“æœ\n")
    
    # è¿›è¡Œå¯¹æ¯”
    comparison = compare_models(model_results)
    
    # ç”ŸæˆæŠ¥å‘Š
    print("ç”Ÿæˆå¯¹æ¯”æŠ¥å‘Š...")
    report = generate_comparison_report(comparison, args.output)
    print(report)
    
    # ç”ŸæˆCSV
    if args.csv:
        df = generate_csv_comparison(comparison, args.csv)
        print(f"\nè¯¦ç»†æ•°æ®é¢„è§ˆ:")
        print(df.head(20))


if __name__ == "__main__":
    main()



